# LihuaCat 未来

## 一句话定义

> **LihuaCat = 🐱 Tabby（理解你）+ StoryBrief（叙事内核）+ 可插拔的呈现层**
> 

LihuaCat 不是视频工具，不是漫画工具。它是一个把「用户的真实照片 + 用户的真实感受」转化成「某种可分享的叙事体验」的系统。

---

## 产品愿景

### 我们在解决什么问题？

大多数人打开相册，看着一堆照片，心里有一团模糊的感觉，但说不出来。"就是觉得那天挺开心的""有点怀念吧"——然后就关掉了。

市面上能做「图片 → 视频」的工具很多（剪映模板、CapCut 一键出片、微信照片电影），但它们解决的是**呈现问题**——怎么把图片排好看。没有人在解决**表达问题**——用户心里那团模糊的感受，怎么变成一个有灵魂的故事。

### LihuaCat 想成为什么？

一个**帮用户搞清楚自己到底想表达什么**的系统。然后把它变成可以分享的叙事体验。

**核心体验承诺**：用户看着最终产出时，会有一个「对，就是这个感觉」的瞬间。

### 和别人不一样的地方

| 维度 | 传统工具 | LihuaCat |
| --- | --- | --- |
| 输入 | 图片 | 图片   **• 用户的感受** |
| 谁决定故事 | AI 或模板 | **用户的感受决定故事**，AI 帮忙表达 |
| 用户做什么 | 选模板、选滤镜、选音乐 | **聊聊天，说说心里话** |
| 产出 | 好看的视频 | **「被理解了」的感觉**  • 承载这种理解的叙事体验 |
| 分享动机 | "看看我做的视频" | **"这就是那天的感觉"** |

### 用户是谁？

暂不锁定具体画像。先从自己出发——Chii 就是第一个用户。

一个直觉：LihuaCat 可能不是一个"随便玩玩"的日常工具，而是一个**在特殊时刻才会打开的东西**——低频但高价值。愿意花时间跟 🐱 Tabby 深聊感受的人，往往是因为那组照片对他们真的很重要。

这个假设需要验证，后续持续更新。

### 怎么验证「被理解」？

最粗暴但最有效的方式：做出来后，用户愿不愿意发给**那个特定的人**看。不是发朋友圈，是发给「那个人」。

### 商业模式

（待探索。但有一个不变的约束——）

- **本地优先**：所有渲染在用户机器上完成
- **用户自带 AI**：AI 能力由用户自己的 Codex/ChatGPT 账号承担
- **开发者零运维**：不跑服务器，不存数据

---

## 地基性的信念

1. **用户真实的图片 + 用户真实的感受 = 有灵魂的叙事体验**
2. **输出形态是可替换的，理解用户的能力是不可替换的**
3. **🐱 Tabby 是产品的灵魂，它的质量决定产品的上限**
4. **本地优先、用户自带 AI、开发者零运维**——这个不变

---

## 执行单元（Agents / Subagents / Tools）

> **命名规则**：Agent使用成年猫科动物命名，拥有独立决策权；Sub-agent 使用年幼猫科动物命名，是 Agent 下属的专项执行器，不具备独立决策权。
> 

### Agents：用 LLM 做判断与生成

### 🐱 Tabby（对话导演）

Tabby 是**唯一与用户直接对话**的 agent：帮助用户把“模糊但真实的感受”变成可确认的表达意图，并在确认后驱动后续生成。

### 🐆 Ocelot（创意总监）

Ocelot 不与用户对话。原为“编剧”，现升级为**创意总监**，统管所有创意层的决策与审稿：

1. 读取 StoryBrief，产出 **CreativePlan**（整体创意方案，含视觉方向 + 音乐意图）
2. 将 CreativePlan 分发给下属 sub-agent（Kitten、Cub）
3. 收集产出，做**统一审稿**：忠实度（是否表达了用户感受）+ 一致性（视觉与音乐是否对齐）
4. 审稿不通过时，给 sub-agent 具体修改指令（改稿制，不重跑；有轮次上限）

> 原 Lynx 的审稿职责已内化为 Ocelot 的“收活审稿”环节，Lynx 不再作为独立 agent 存在。
> 

### Sub-agents：Agent 下属的专项执行器

### 🐾 Kitten（视觉脚本）

从 Ocelot 拆出的视觉脚本执行器：接收 CreativePlan 中的视觉方向 + 图片列表，产出 VisualScript（分镜顺序、时长、转场、字幕文案、情绪节奏）。

### 🐾 Cub（音乐）

新增的音乐执行器：接收 CreativePlan 中的音乐意图，产出 MIDI 文件（纯音乐）。生成方式：文本描述 → MIDI，具体技术路径后续确定。

### StoryBrief 生成器（叙事资产整理）

把对话中分散的信息整理成结构化叙事资产，作为后续脚本与呈现层的稳定输入。

### Tools：确定性的本地能力

- 素材收集与校验、压缩/入库
- 本地渲染（可插拔 renderer）
- MIDI → SoundFont 合成音频（本地闭合）
- 产物落盘与日志/可审阅资产归档
- 鉴权/模型调用封装（让“用户自带 AI”成为可控输入）

---

## 路线图（只保留“未来/待验证”部分）

### 近期：把“被理解”的命中率做上去（持续）

- **Tabby 追问与收束**：更自然、更少轮次、更少“审讯感”；让用户更容易说出那句关键的“原话”
- **确认页体验**：让用户能明确地说“对 / 不对”，并能在不填表的前提下快速修正方向
- **脚本质量稳定性**：同一份意图下，脚本应该可预测；出戏/俗套/违背 avoidance 的概率要持续下降
- **可选审稿闭环**：审稿的目标是“忠实表达”，而不是“文学性更强”；审稿意见要可执行、可收敛

### 中期：呈现层可插拔（不稀释叙事内核）

- **更多 Renderer**：漫画、AVP/沉浸式、长图等
- **音乐生成进化**：更丰富的音色、配器、风格选择；用户可自选 SoundFont；探索专用本地音乐模型

### 长期：把“你”当成持续存在的人

- **记忆系统**：跨会话记住用户的表达偏好、历史故事与敏感点（默认谨慎、可控、可关闭）
- **动态素材**：从纯静态图片扩展到 Live Photo / 短视频片段（输入、叙事与渲染都要一起升级）

---

## 非目标（至少在现阶段）

- 纯“图片拼模板出片”的产品形态（那不是 LihuaCat）
- 云端存储用户照片/对话（违背本地优先）
- 用更多花活掩盖“没理解用户”的问题